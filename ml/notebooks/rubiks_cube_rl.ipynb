{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubik's Cube Reinforcement Learning Solver\n",
    "\n",
    "This notebook demonstrates how to build a reinforcement learning agent to solve a simplified Rubik's Cube.\n",
    "\n",
    "## Overview\n",
    "- Create a Rubik's Cube environment\n",
    "- Implement Deep Q-Network (DQN)\n",
    "- Train the agent\n",
    "- Visualize the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rubik's Cube Environment\n",
    "\n",
    "We'll implement a simplified 2x2x2 cube (Pocket Cube) for easier learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RubiksCube2x2:\n",
    "    \"\"\"\n",
    "    2x2x2 Rubik's Cube (Pocket Cube) environment.\n",
    "    \n",
    "    Faces: 0=Front, 1=Back, 2=Up, 3=Down, 4=Left, 5=Right\n",
    "    Colors: 0-5 representing each face color\n",
    "    \"\"\"\n",
    "    \n",
    "    # Actions: each face can be rotated clockwise or counter-clockwise\n",
    "    ACTIONS = ['F', \"F'\", 'B', \"B'\", 'U', \"U'\", 'D', \"D'\", 'L', \"L'\", 'R', \"R'\"]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to solved state.\"\"\"\n",
    "        # Each face is a 2x2 array with the same color\n",
    "        self.state = np.array([np.full((2, 2), i) for i in range(6)])\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Return flattened state for neural network.\"\"\"\n",
    "        return self.state.flatten().astype(np.float32) / 5.0\n",
    "    \n",
    "    def is_solved(self):\n",
    "        \"\"\"Check if cube is solved.\"\"\"\n",
    "        for face in self.state:\n",
    "            if not np.all(face == face[0, 0]):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _rotate_face(self, face_idx, clockwise=True):\n",
    "        \"\"\"Rotate a face 90 degrees.\"\"\"\n",
    "        if clockwise:\n",
    "            self.state[face_idx] = np.rot90(self.state[face_idx], -1)\n",
    "        else:\n",
    "            self.state[face_idx] = np.rot90(self.state[face_idx], 1)\n",
    "    \n",
    "    def move(self, action_idx):\n",
    "        \"\"\"Execute a move.\"\"\"\n",
    "        action = self.ACTIONS[action_idx]\n",
    "        clockwise = \"'\" not in action\n",
    "        face = action[0]\n",
    "        \n",
    "        if face == 'F':\n",
    "            self._move_front(clockwise)\n",
    "        elif face == 'B':\n",
    "            self._move_back(clockwise)\n",
    "        elif face == 'U':\n",
    "            self._move_up(clockwise)\n",
    "        elif face == 'D':\n",
    "            self._move_down(clockwise)\n",
    "        elif face == 'L':\n",
    "            self._move_left(clockwise)\n",
    "        elif face == 'R':\n",
    "            self._move_right(clockwise)\n",
    "    \n",
    "    def _move_front(self, clockwise=True):\n",
    "        \"\"\"Rotate front face.\"\"\"\n",
    "        self._rotate_face(0, clockwise)\n",
    "        temp = self.state[2][1, :].copy()\n",
    "        if clockwise:\n",
    "            self.state[2][1, :] = self.state[4][:, 1][::-1]\n",
    "            self.state[4][:, 1] = self.state[3][0, :]\n",
    "            self.state[3][0, :] = self.state[5][:, 0][::-1]\n",
    "            self.state[5][:, 0] = temp\n",
    "        else:\n",
    "            self.state[2][1, :] = self.state[5][:, 0]\n",
    "            self.state[5][:, 0] = self.state[3][0, :][::-1]\n",
    "            self.state[3][0, :] = self.state[4][:, 1]\n",
    "            self.state[4][:, 1] = temp[::-1]\n",
    "    \n",
    "    def _move_back(self, clockwise=True):\n",
    "        \"\"\"Rotate back face.\"\"\"\n",
    "        self._rotate_face(1, clockwise)\n",
    "        temp = self.state[2][0, :].copy()\n",
    "        if clockwise:\n",
    "            self.state[2][0, :] = self.state[5][:, 1]\n",
    "            self.state[5][:, 1] = self.state[3][1, :][::-1]\n",
    "            self.state[3][1, :] = self.state[4][:, 0]\n",
    "            self.state[4][:, 0] = temp[::-1]\n",
    "        else:\n",
    "            self.state[2][0, :] = self.state[4][:, 0][::-1]\n",
    "            self.state[4][:, 0] = self.state[3][1, :]\n",
    "            self.state[3][1, :] = self.state[5][:, 1][::-1]\n",
    "            self.state[5][:, 1] = temp\n",
    "    \n",
    "    def _move_up(self, clockwise=True):\n",
    "        \"\"\"Rotate up face.\"\"\"\n",
    "        self._rotate_face(2, clockwise)\n",
    "        temp = self.state[0][0, :].copy()\n",
    "        if clockwise:\n",
    "            self.state[0][0, :] = self.state[5][0, :]\n",
    "            self.state[5][0, :] = self.state[1][0, :]\n",
    "            self.state[1][0, :] = self.state[4][0, :]\n",
    "            self.state[4][0, :] = temp\n",
    "        else:\n",
    "            self.state[0][0, :] = self.state[4][0, :]\n",
    "            self.state[4][0, :] = self.state[1][0, :]\n",
    "            self.state[1][0, :] = self.state[5][0, :]\n",
    "            self.state[5][0, :] = temp\n",
    "    \n",
    "    def _move_down(self, clockwise=True):\n",
    "        \"\"\"Rotate down face.\"\"\"\n",
    "        self._rotate_face(3, clockwise)\n",
    "        temp = self.state[0][1, :].copy()\n",
    "        if clockwise:\n",
    "            self.state[0][1, :] = self.state[4][1, :]\n",
    "            self.state[4][1, :] = self.state[1][1, :]\n",
    "            self.state[1][1, :] = self.state[5][1, :]\n",
    "            self.state[5][1, :] = temp\n",
    "        else:\n",
    "            self.state[0][1, :] = self.state[5][1, :]\n",
    "            self.state[5][1, :] = self.state[1][1, :]\n",
    "            self.state[1][1, :] = self.state[4][1, :]\n",
    "            self.state[4][1, :] = temp\n",
    "    \n",
    "    def _move_left(self, clockwise=True):\n",
    "        \"\"\"Rotate left face.\"\"\"\n",
    "        self._rotate_face(4, clockwise)\n",
    "        temp = self.state[0][:, 0].copy()\n",
    "        if clockwise:\n",
    "            self.state[0][:, 0] = self.state[2][:, 0]\n",
    "            self.state[2][:, 0] = self.state[1][:, 1][::-1]\n",
    "            self.state[1][:, 1] = self.state[3][:, 0][::-1]\n",
    "            self.state[3][:, 0] = temp\n",
    "        else:\n",
    "            self.state[0][:, 0] = self.state[3][:, 0]\n",
    "            self.state[3][:, 0] = self.state[1][:, 1][::-1]\n",
    "            self.state[1][:, 1] = self.state[2][:, 0][::-1]\n",
    "            self.state[2][:, 0] = temp\n",
    "    \n",
    "    def _move_right(self, clockwise=True):\n",
    "        \"\"\"Rotate right face.\"\"\"\n",
    "        self._rotate_face(5, clockwise)\n",
    "        temp = self.state[0][:, 1].copy()\n",
    "        if clockwise:\n",
    "            self.state[0][:, 1] = self.state[3][:, 1]\n",
    "            self.state[3][:, 1] = self.state[1][:, 0][::-1]\n",
    "            self.state[1][:, 0] = self.state[2][:, 1][::-1]\n",
    "            self.state[2][:, 1] = temp\n",
    "        else:\n",
    "            self.state[0][:, 1] = self.state[2][:, 1]\n",
    "            self.state[2][:, 1] = self.state[1][:, 0][::-1]\n",
    "            self.state[1][:, 0] = self.state[3][:, 1][::-1]\n",
    "            self.state[3][:, 1] = temp\n",
    "    \n",
    "    def scramble(self, num_moves=10):\n",
    "        \"\"\"Scramble the cube with random moves.\"\"\"\n",
    "        moves = []\n",
    "        for _ in range(num_moves):\n",
    "            action = random.randint(0, len(self.ACTIONS) - 1)\n",
    "            self.move(action)\n",
    "            moves.append(self.ACTIONS[action])\n",
    "        return moves\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Print the cube state.\"\"\"\n",
    "        face_names = ['Front', 'Back', 'Up', 'Down', 'Left', 'Right']\n",
    "        colors = ['G', 'B', 'W', 'Y', 'O', 'R']  # Green, Blue, White, Yellow, Orange, Red\n",
    "        \n",
    "        for i, name in enumerate(face_names):\n",
    "            print(f\"{name}:\")\n",
    "            for row in self.state[i]:\n",
    "                print(' '.join(colors[int(c)] for c in row))\n",
    "            print()\n",
    "\n",
    "# Test the cube\n",
    "cube = RubiksCube2x2()\n",
    "print(\"Solved cube:\")\n",
    "cube.visualize()\n",
    "print(f\"Is solved: {cube.is_solved()}\")\n",
    "\n",
    "moves = cube.scramble(5)\n",
    "print(f\"\\nAfter scrambling with: {moves}\")\n",
    "cube.visualize()\n",
    "print(f\"Is solved: {cube.is_solved()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Rubik's Cube.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_sizes=[256, 256, 128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_size = state_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size)\n",
    "            ])\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(input_size, action_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Test the network\n",
    "state_size = 24  # 6 faces * 4 squares\n",
    "action_size = 12  # 12 possible moves\n",
    "model = DQN(state_size, action_size).to(device)\n",
    "print(model)\n",
    "print(f'\\nParameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*transitions))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent for solving Rubik's Cube.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        lr=1e-4,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=100000,\n",
    "        batch_size=64,\n",
    "        target_update=10\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        self.steps = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            self.policy_net.eval()\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            self.policy_net.train()\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(batch.state)).to(device)\n",
    "        actions = torch.LongTensor(batch.action).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(batch.next_state)).to(device)\n",
    "        rewards = torch.FloatTensor(batch.reward).to(device)\n",
    "        dones = torch.FloatTensor(batch.done).to(device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.policy_net(states).gather(1, actions).squeeze()\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.smooth_l1_loss(current_q, target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, num_episodes=1000, max_steps=50, scramble_depth=3):\n",
    "    \"\"\"Train the DQN agent.\"\"\"\n",
    "    cube = RubiksCube2x2()\n",
    "    \n",
    "    history = {\n",
    "        'episode_rewards': [],\n",
    "        'episode_lengths': [],\n",
    "        'solved': [],\n",
    "        'losses': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Reset and scramble\n",
    "        cube.reset()\n",
    "        cube.scramble(scramble_depth)\n",
    "        state = cube.get_state()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and execute action\n",
    "            action = agent.select_action(state)\n",
    "            cube.move(action)\n",
    "            next_state = cube.get_state()\n",
    "            \n",
    "            # Calculate reward\n",
    "            if cube.is_solved():\n",
    "                reward = 100\n",
    "                done = True\n",
    "            else:\n",
    "                reward = -1  # Small penalty for each move\n",
    "                done = False\n",
    "            \n",
    "            # Store transition\n",
    "            agent.memory.push(state, action, next_state, reward, done)\n",
    "            \n",
    "            # Train\n",
    "            loss = agent.train_step()\n",
    "            episode_loss += loss\n",
    "            episode_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record history\n",
    "        history['episode_rewards'].append(episode_reward)\n",
    "        history['episode_lengths'].append(step + 1)\n",
    "        history['solved'].append(cube.is_solved())\n",
    "        history['losses'].append(episode_loss / (step + 1) if step > 0 else 0)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            recent_solved = sum(history['solved'][-100:])\n",
    "            avg_reward = np.mean(history['episode_rewards'][-100:])\n",
    "            print(f'Episode {episode + 1}/{num_episodes} - '\n",
    "                  f'Solved: {recent_solved}/100 - '\n",
    "                  f'Avg Reward: {avg_reward:.2f} - '\n",
    "                  f'Epsilon: {agent.epsilon:.3f}')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent and train\n",
    "state_size = 24\n",
    "action_size = 12\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.1,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_size=50000,\n",
    "    batch_size=64,\n",
    "    target_update=100\n",
    ")\n",
    "\n",
    "# Train (you may need to increase episodes for better results)\n",
    "history = train_agent(agent, num_episodes=500, max_steps=20, scramble_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Episode rewards\n",
    "window = 50\n",
    "rewards_smooth = np.convolve(history['episode_rewards'], np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(rewards_smooth)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].set_title('Episode Rewards (Smoothed)')\n",
    "\n",
    "# Solve rate\n",
    "solved_smooth = np.convolve([int(s) for s in history['solved']], np.ones(window)/window, mode='valid')\n",
    "axes[0, 1].plot(solved_smooth * 100)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Solve Rate (%)')\n",
    "axes[0, 1].set_title('Solve Rate (Smoothed)')\n",
    "\n",
    "# Episode lengths\n",
    "lengths_smooth = np.convolve(history['episode_lengths'], np.ones(window)/window, mode='valid')\n",
    "axes[1, 0].plot(lengths_smooth)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Steps')\n",
    "axes[1, 0].set_title('Episode Lengths (Smoothed)')\n",
    "\n",
    "# Loss\n",
    "losses_smooth = np.convolve(history['losses'], np.ones(window)/window, mode='valid')\n",
    "axes[1, 1].plot(losses_smooth)\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].set_title('Training Loss (Smoothed)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, num_tests=10, scramble_depth=3, max_steps=20):\n",
    "    \"\"\"Test the trained agent.\"\"\"\n",
    "    cube = RubiksCube2x2()\n",
    "    results = []\n",
    "    \n",
    "    for test in range(num_tests):\n",
    "        cube.reset()\n",
    "        scramble_moves = cube.scramble(scramble_depth)\n",
    "        print(f\"\\nTest {test + 1}: Scrambled with {scramble_moves}\")\n",
    "        \n",
    "        state = cube.get_state()\n",
    "        solution = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            cube.move(action)\n",
    "            solution.append(cube.ACTIONS[action])\n",
    "            state = cube.get_state()\n",
    "            \n",
    "            if cube.is_solved():\n",
    "                print(f\"  Solved in {step + 1} moves: {solution}\")\n",
    "                results.append(True)\n",
    "                break\n",
    "        else:\n",
    "            print(f\"  Failed to solve in {max_steps} moves\")\n",
    "            results.append(False)\n",
    "    \n",
    "    print(f\"\\nSuccess rate: {sum(results)}/{num_tests} ({100*sum(results)/num_tests:.1f}%)\")\n",
    "\n",
    "# Set agent to evaluation mode\n",
    "agent.epsilon = 0  # No exploration during testing\n",
    "test_agent(agent, num_tests=10, scramble_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'policy_net': agent.policy_net.state_dict(),\n",
    "    'target_net': agent.target_net.state_dict(),\n",
    "    'optimizer': agent.optimizer.state_dict(),\n",
    "}, 'rubiks_cube_dqn.pth')\n",
    "print('Model saved to rubiks_cube_dqn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Increase Training**: Train for more episodes with deeper scrambles\n",
    "2. **Curriculum Learning**: Start with simple scrambles and increase difficulty\n",
    "3. **Better Reward Shaping**: Add intermediate rewards for partial solves\n",
    "4. **Double DQN**: Use double Q-learning for more stable training\n",
    "5. **Dueling DQN**: Separate value and advantage streams\n",
    "6. **3x3x3 Cube**: Scale up to standard Rubik's Cube\n",
    "7. **AlphaZero Approach**: Combine MCTS with neural networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
